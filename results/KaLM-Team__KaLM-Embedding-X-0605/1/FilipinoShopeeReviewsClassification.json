{
  "dataset_revision": "d096f402fdc76886458c0cfb5dedc829bea2b935",
  "task_name": "FilipinoShopeeReviewsClassification",
  "mteb_version": "1.38.9",
  "scores": {
    "validation": [
      {
        "accuracy": 0.426123,
        "f1": 0.39365,
        "f1_weighted": 0.393628,
        "scores_per_experiment": [
          {
            "accuracy": 0.430664,
            "f1": 0.409185,
            "f1_weighted": 0.409172
          },
          {
            "accuracy": 0.424316,
            "f1": 0.381726,
            "f1_weighted": 0.381729
          },
          {
            "accuracy": 0.445312,
            "f1": 0.40731,
            "f1_weighted": 0.407275
          },
          {
            "accuracy": 0.42627,
            "f1": 0.393483,
            "f1_weighted": 0.393445
          },
          {
            "accuracy": 0.412598,
            "f1": 0.366787,
            "f1_weighted": 0.366754
          },
          {
            "accuracy": 0.419434,
            "f1": 0.375659,
            "f1_weighted": 0.37563
          },
          {
            "accuracy": 0.425293,
            "f1": 0.386839,
            "f1_weighted": 0.386809
          },
          {
            "accuracy": 0.429199,
            "f1": 0.402761,
            "f1_weighted": 0.402751
          },
          {
            "accuracy": 0.432617,
            "f1": 0.416961,
            "f1_weighted": 0.416906
          },
          {
            "accuracy": 0.415527,
            "f1": 0.395793,
            "f1_weighted": 0.395806
          }
        ],
        "main_score": 0.426123,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ],
    "test": [
      {
        "accuracy": 0.43374,
        "f1": 0.403639,
        "f1_weighted": 0.403625,
        "scores_per_experiment": [
          {
            "accuracy": 0.438477,
            "f1": 0.417717,
            "f1_weighted": 0.417705
          },
          {
            "accuracy": 0.426758,
            "f1": 0.387958,
            "f1_weighted": 0.387971
          },
          {
            "accuracy": 0.439453,
            "f1": 0.401225,
            "f1_weighted": 0.401197
          },
          {
            "accuracy": 0.426758,
            "f1": 0.393533,
            "f1_weighted": 0.393508
          },
          {
            "accuracy": 0.433594,
            "f1": 0.393734,
            "f1_weighted": 0.393693
          },
          {
            "accuracy": 0.425293,
            "f1": 0.384244,
            "f1_weighted": 0.384229
          },
          {
            "accuracy": 0.449219,
            "f1": 0.414001,
            "f1_weighted": 0.413977
          },
          {
            "accuracy": 0.436035,
            "f1": 0.410009,
            "f1_weighted": 0.410008
          },
          {
            "accuracy": 0.457031,
            "f1": 0.446771,
            "f1_weighted": 0.44674
          },
          {
            "accuracy": 0.404785,
            "f1": 0.387201,
            "f1_weighted": 0.387227
          }
        ],
        "main_score": 0.43374,
        "hf_subset": "default",
        "languages": [
          "fil-Latn"
        ]
      }
    ]
  },
  "evaluation_time": 53.35766077041626,
  "kg_co2_emissions": null
}